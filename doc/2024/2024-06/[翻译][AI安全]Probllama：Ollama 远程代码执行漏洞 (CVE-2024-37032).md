#  [翻译][AI安全]Probllama：Ollama 远程代码执行漏洞 (CVE-2024-37032)   
walker1995  沃克学安全   2024-06-28 20:43  
  
原文地址：  
https://www.wiz.io/blog/probllama-ollama-vulnerability-cve-2024-37032  
  
发表时间：2024年6月24日  
  
作者：[  
Sagi Tzadik](  
https://www.wiz.io/authors/sagi)  
  
  
**Wiz Research 发现了开源 AI 基础设施项目 Ollama 中一个易于利用的远程代码执行漏洞 CVE-2024-37032**  
### 简介与概述   
  
[Ollama](  
https://ollama.com/)  
是最流行的运行 AI 模型的开源项目之一，在[  
GitHub](  
https://github.com/ollama/ollama)  
上有超过 7 万star，在[  
Docker Hub](  
https://hub.docker.com/r/ollama/ollama)  
上每月有数十万次pull。受 Docker 的启发，Ollama 旨在简化打包和部署 AI 模型的过程。   
  
Wiz Research 在 Ollama 中发现了一个易于利用的远程代码执行漏洞：CVE-2024-37032，称为“Probllama”。此安全问题已负责任地披露给 Ollama 的维护人员，并且已得到缓解。建议 Ollama 用户将其 Ollama 安装升级到 0.1.34 或更高版本。   
  
我们的研究表明，截至 6 月 10 日，有大量Ollama 实例暴露在互联网上，它们运行在易受攻击版本。在这篇博文中，我们将详细介绍我们发现的内容和发现过程，以及组织可以采取的缓解技术和预防措施。   
### 人工智能安全要点   
  
总的来说，鉴于 Wiz Research 团队持续关注人工智能系统固有的风险，我们的研究结果强调了这样一个事实：人们更关注人工智能的变革力量及其彻底改变商业运作方式的潜力，而[  
人工智能安全措施](  
https://www.wiz.io/academy/ai-security-best-practices)  
在很大程度上被忽视了。   
  
各组织正在迅速采用各种新的 AI 工具和基础设施，以期提高竞争优势。这些工具通常处于开发的早期阶段，缺乏标准化的安全功能，比如身份验证。此外，由于它们的代码库较新，使得找到关键软件漏洞相对容易，从而使其成为潜在威胁行为者的完美目标。这是我们发现中反复出现的主题——请参阅 Wiz Research 之前对 AI 即服务提供商[  
Hugging Face](  
https://www.wiz.io/blog/wiz-and-hugging-face-address-risks-to-ai-infrastructure)  
和[  
Replicate](  
https://www.wiz.io/blog/wiz-research-discovers-critical-vulnerability-in-replicate)  
的研究，以及我们的[  
《云端 AI 现状》  
报告](  
https://www.wiz.io/blog/key-findings-from-the-state-of-ai-in-the-cloud-report-2024)  
和去年发现的AI 研究人员意外泄露的 [  
38TB 数据](  
https://www.wiz.io/blog/38-terabytes-of-private-data-accidentally-exposed-by-microsoft-ai-researchers)  
。   
  
过去一年，推理服务器中发现了多个远程代码执行 (RCE) 漏洞，包括 TorchServe、Ray Anyscale 和 Ollama。这些漏洞可能允许攻击者接管自托管的 AI 推理服务器，窃取或修改 AI 模型，并破坏 AI 应用程序。   
  
关键问题不仅在于漏洞本身，还在于这些新工具本身缺乏身份验证支持。如果暴露在互联网上，任何攻击者都可以连接到它们，窃取或修改 AI 模型，甚至可以作为内置功能执行远程代码（见TorchServe 和[  
Ray Anyscale](  
https://www.oligo.security/blog/shadowray-attack-ai-workloads-actively-exploited-in-the-wild)  
）。缺乏身份验证支持意味着这些工具绝不应该在没有保护性中间件（例如具有身份验证的反向代理）的情况下暴露在外部。尽管如此，当我们在互联网上扫描暴露的 Ollama 服务器时，我们发现有 1,000 多个暴露的实例托管着大量 AI 模型，包括 Ollama 公共存储库中未列出的私有模型，这凸显出一个重大的安全漏洞。   
### 缓解和检测   
  
要利用此漏洞，攻击者必须向 Ollama API 服务器发送特制的 HTTP 请求。在[  
默认的 Linux 安装](  
https://github.com/ollama/ollama/?tab=readme-ov-file#linux)  
中，API 服务绑定到 localhost，这大大降低了远程利用风险。然而，在  
**docker 部署**  
（[  
ollama/ollama](  
https://hub.docker.com/r/ollama/ollama)  
）中，  
**API 服务是公开暴露的**  
，因此可以被远程利用。   
  
Wiz 客户可以使用 Wiz 威胁中心中预先构建的查询和建议来搜索其环境中易受攻击的实例。   
### 解释和技术说明   
#### 为什么要研究Ollama？   
  
我们的研究团队积极努力为人工智能服务、工具和基础设施的安全做出贡献，并且我们也在研究工作中使用人工智能。   
  
对于另一个项目，我们希望利用大上下文 AI 模型。幸运的是，大约在那个时候，[  
Gradient 发布了他们的 Llama3 版本，该版本具有 100 万个 token 的上下文](  
https://huggingface.co/gradientai/Llama-3-70B-Instruct-Gradient-1048k)  
。   
  
Ollama是最受欢迎的运行 AI 模型的开源项目之一，在  
GitHub  
上有超过 7 万star，在  
Docker Hub  
上每月有数十万次pull，Ollama 似乎是自托管[  
此模型](  
https://ollama.com/library/llama3-gradient)的  
最简单方法😊。   
  
![](https://mmbiz.qpic.cn/mmbiz_jpg/CFEQEjGaicCOKbU03j97TokUA8aHOPA5rnjd3rKN5IGWftvBiarx3JDY7B7PyR2oBwqtvAv23jMvMV1mdicJLfs1g/640?wx_fmt=other&from=appmsg "")  
#### Ollama 架构   
  
Ollama 由两个主要组件组成：客户端和服务器。服务器公开[  
多个 API](  
https://github.com/ollama/ollama/blob/main/docs/api.md)  
来执行核心功能，例如从注册表中拉取模型、针对给定提示生成预测等。客户端是用户与之交互的部分（即前端），例如 CLI（命令行界面）。   
  
在试验 Ollama 时，我们的团队在 Ollama 服务器中发现了一个严重的安全漏洞。由于输入验证不足，可以利用[  
路径遍历](  
https://owasp.org/www-community/attacks/Path_Traversal)  
漏洞任意覆盖服务器上的文件。正如我们在下面演示的那样，这可以进一步利用为完整的远程代码执行。   
  
该问题在 Docker 安装中极其严重，因为服务以  
root  
特权运行并默认进行监听  
0.0.0.0  
- 这使得该漏洞可以被远程利用。   
  
![](https://mmbiz.qpic.cn/mmbiz_jpg/CFEQEjGaicCOKbU03j97TokUA8aHOPA5rIY8VQibs8zNibxah6vfMW34gveQ2yicm0a82XufWBLvW1MLJvicGexoeow/640?wx_fmt=other&from=appmsg "")  
  
值得一提的是，Ollama 不支持开箱即用的身份验证。如果用户决定公开其安装，通常建议将[  
Ollama 部署在反向代理后面](  
https://github.com/ollama/ollama/issues/849)  
以强制执行身份验证。实际上，我们的研究表明，有大量安装在没有任何身份验证的情况下暴露在互联网上。   
#### 漏洞：通过路径遍历进行任意文件写入   
  
Ollama 的 HTTP 服务暴露执行各种操作的[  
多个 API 端点](  
https://github.com/ollama/ollama/blob/main/docs/api.md)  
。  
  
![](https://mmbiz.qpic.cn/mmbiz_jpg/CFEQEjGaicCOKbU03j97TokUA8aHOPA5ricwnbEPKjhaH0RX7l0b02vWcL5wGHRjzH08GkjKTISBqJT2ldqB0oEQ/640?wx_fmt=other&from=appmsg "")  
  
其中一个端点  
/api/pull  
可用于从 Ollama 注册表下载模型。   
  
默认情况下，模型是从 Ollama 的官方注册表 (   
registry.ollama.com  
) 下载的，但是，也可以从私人注册表中获取模型。  
  
![](https://mmbiz.qpic.cn/mmbiz_jpg/CFEQEjGaicCOKbU03j97TokUA8aHOPA5r8KzdXpGnKmgFf6CERoJA1sUicpAHqVbtVKPxZzOf8DpLMicseF29nO4A/640?wx_fmt=other&from=appmsg "")  
从私有注册表中拉取模型  
虽然 Ollama 的官方注册表可以被视为“可信的”，但任何人都可以建立自己的注册表并在其上托管模型。作为研究人员，我们对这个攻击面很感兴趣——私人注册表是否被盲目信任？恶意的私人注册表会造成什么危害？   
  
我们发现，当从私人注册表中拉取模型时（通过查询  
http://[victim]:11434/api/pull  
API 端点），有可能在  
digest  
字段中提供一个包含路径遍历攻击载荷的恶意清单文件。   
  
例如：   
```
{
  "schemaVersion": 2,
  "mediaType": "application/vnd.docker.distribution.manifest.v2+json",
  "config": {
    "mediaType": "application/vnd.docker.container.image.v1+json",
    "digest": "../../../../../../../../../../../../../../../../../../../traversal",
    "size": 5
  },
  "layers": [
    {
      "mediaType": "application/vnd.ollama.image.license",
      "digest": "../../../../../../../../../../../../../../../../../../../../../traversal",
      "size": 7020
    }
  ]
}
```  
  
给定层的  
digest  
字段应等于该层的哈希值。除此之外，该层的  
digest  
字段还用于将模型文件存储在磁盘上：   
```
/root/.ollama/models/blobs/sha256-04778965089b91318ad61d0995b7e44fad4b9a9f4e049d7be90932bf8812e828
```  
  
然而，我们发现该  
digest  
字段的使用没有经过适当的验证，导致在尝试将其存储在文件系统时发生路径遍历。此问题可被用来破坏系统上的任意文件。   
#### 实现任意文件读取   
  
通过利用上一个问题，我们可以在服务器上植入一个额外的恶意清单文件（例如  
/root/.ollama/models/manifests/%ATTACKER_IP%/library/manifest/latest  
），从而有效地将新模型注册到服务器。我们发现，如果我们的模型清单包含其某一层  
digest  
字段的路径遍历payload，则在尝试通过端点  
http://[victim]:11434/api/push  
将模型push到远程注册表时，服务器将泄露  
digest  
字段中指定的文件的内容。   
  
![](https://mmbiz.qpic.cn/mmbiz_jpg/CFEQEjGaicCOKbU03j97TokUA8aHOPA5r4GlKqAMWs5NFHeaiazvVRTpobicEv4Sk6YzokBicfzfJLo1cjo9lmyTOg/640?wx_fmt=other&from=appmsg "")  
#### 最后，远程代码执行   
  
正如我们之前提到的，可以利用任意文件写入漏洞来破坏系统中的某些文件。在 Docker 安装中，利用该漏洞并实现[  
远程代码执行](  
https://www.wiz.io/academy/remote-code-execution-rce-attack)  
非常简单，因为服务以  
root  
特权运行。   
  
我们认为实现远程代码执行的最简单方法是破坏[   
ld.so  
](  
https://man7.org/linux/man-pages/man8/ld.so.8.html)  
配置文件，具体来说是  
/etc/ld.so.preload  
。此文件包含一个空格分隔的共享库列表，这些共享库应在启动新进程时加载。使用我们的任意文件写入漏洞利用原语，我们将有效载荷作为共享库植入文件系统（  
/root/bad.so  
），然后我们破坏  
etc/ld.so.preload  
以包含它。最后，我们在 Ollama API 服务上查询端点  
/api/chat  
，随后创建一个新进程并加载我们的有效载荷！  
  
关于未使用  
root  
特权运行实例的漏洞利用 - 我们确实有一个利用 /Arbitrary File Read 原语的漏洞利用策略。不过，这将留给读者练习😊   
  
演示视频地址：  
```
https://www.youtube.com/watch?v=nINPZbsf0-g
```  
### 结论   
  
 CVE-2024-37032 是一个易于利用的远程代码执行漏洞，会影响现代 AI 基础设施。尽管代码库相对较新且采用现代编程语言编写，但  
路径遍历  
等经典漏洞仍然会存在。   
  
安全团队应将其 Ollama 实例更新至最新版本以缓解此漏洞。此外，建议不要将 Ollama 暴露在互联网上，除非它受到某种身份验证机制（如反向代理）的保护。   
### 负责任的披露时间线  
  
我们于 2024 年 5 月负责任地向 Ollama 的开发团队披露了此漏洞。Ollama 迅速调查并解决了该问题，同时向我们通报了最新情况。   
- **2024 年 5 月 5 日**  
——Wiz Research 向 Ollama 报告了该问题。  
  
- **2024 年 5 月 5 日**  
——Ollama 确认收到报告。   
  
- **2024 年 5 月 5 日**  
——Ollama 通知 Wiz Research，他们已向 GitHub 提交了修复程序。   
  
- **2024 年 5 月 8 日**  
——Ollama 发布修补版本。   
  
- **2024 年 6 月 24 日**  
——Wiz Research 发表了一篇关于此问题的博客。   
  
Ollama 在收到我们的初步报告后大约 4 小时内就做出了修复，展现了令人印象深刻的响应时间和对产品安全的承诺。   
  
  
如果喜欢小编的文章，记得多多转发，点赞+关注支持一下哦~，您的点赞和支持是我最大的动力~  
  
   
  
