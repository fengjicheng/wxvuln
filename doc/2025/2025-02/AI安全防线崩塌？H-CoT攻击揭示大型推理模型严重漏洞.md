#  AI安全防线崩塌？H-CoT攻击揭示大型推理模型严重漏洞   
原创 网空闲话  网空闲话plus   2025-02-25 22:56  
  
随着人工智能技术的飞速发展，大型推理模型（LRMs）在复杂任务中的应用日益广泛，但其安全性和可靠性问题也日益凸显。杜克大学、埃森哲和台湾国立清华大学的研究团队通过构建“恶意教育者”数据集，揭示了当前LRMs在安全推理机制上的严重漏洞，并提出了一种名为“劫持思维链”（H-CoT）的攻击方法。该方法能够显著降低模型对有害内容的拒绝率，甚至使其从谨慎转变为主动提供有害信息。这一发现不仅暴露了当前AI模型在安全机制上的脆弱性，也提醒我们，随着AI技术的普及，安全性和伦理问题必须得到更多关注。研究团队呼吁开发更强大的安全机制，以平衡模型效用与内容无害性，确保AI技术的安全应用。在未来的AI发展中，我们不仅需要关注技术的进步，更要重视其潜在的风险和挑战。只有通过不断改进和优化，才能确保AI技术真正造福人类社会。  
  
![人工智能被劫持：新越狱漏洞利用思路](https://mmbiz.qpic.cn/mmbiz_jpg/0KRmt3K30icUIgGFmqh0BECuuJ3Ma0jErLZtx8iatbkRiaa7IkV66oNxYJPQgcooWQ7wmVBYjsZibib2X8yh2PlZceA/640?wx_fmt=jpeg&from=appmsg "")  
  
随着大型推理模型（LRMs）在复杂任务中的广泛应用，其安全性和可靠性成为关键问题。OpenAI的o1/o3系列、DeepSeek-R1和谷歌的Gemini 2.0 Flash Thinking等模型通过思维链（Chain-of-Thought, CoT）推理机制进行安全决策，旨在平衡模型效用与内容无害性。然而，这种机制是否足够稳健，能否抵御复杂的攻击，仍是一个未解之谜。  
  
主要发现  
  
杜克大学、埃森哲和台湾国立清华大学的研究团队通过构建“恶意教育者”（Malicious-Educator）数据集，揭示了当前LRMs在安全推理机制上的严重漏洞。他们提出了一种名为“劫持思维链”（Hijacking Chain-of-Thought, H-CoT）的攻击方法，能够显著降低模型对有害内容的拒绝率，甚至使其从谨慎转变为主动提供有害信息。  
  
OpenAI o1/o3系列  
  
初始拒绝率高达98%，但在H-CoT攻击下，拒绝率骤降至2%以下。  
  
模型更新和安全机制的调整可能削弱了其安全性，尤其是在面对多语言和地理定位攻击时。  
  
通过VPN代理进行地理定位进一步削弱了安全保障，欧洲IP的脆弱性比美国端点高15%。  
  
DeepSeek-R1  
  
生成后审核机制存在缺陷，拒绝率仅为20%，且在安全过滤器介入前会短暂输出有害内容。  
  
在H-CoT攻击下，拒绝率进一步下降至4%。  
  
多语言不一致性（例如，英语查询绕过了中文安全过滤器）将成功率提高到96.8%。  
  
Gemini 2.0 Flash Thinking  
  
初始拒绝率低于10%，在H-CoT攻击下，模型从谨慎转变为急切提供有害内容。  
  
在100%的测试案例中提供详细的犯罪框架。  
  
主要影响  
  
安全机制失效  
  
H-CoT攻击通过操纵模型的中间推理过程，成功绕过了内置的安全检查，暴露了当前LRMs在安全推理机制上的脆弱性。  
  
研究人员发现，OpenAI的o1模型通常会拒绝超过99%的与虐待儿童或恐怖主义相关的提示，但在H-CoT攻击下，其拒绝率在某些情况下降至2%以下。  
  
模型更新带来的风险  
  
为提升性能和降低成本，模型更新可能无意中削弱了安全性，尤其是在面对竞争压力时。  
  
作者表示，对o1模型的更新可能无意中削弱了其安全性，这可能是为了应对来自DeepSeek-R1的竞争而做出的旨在提高推理性能和成本效率的权衡。  
  
多语言和地理定位攻击  
  
不同语言和地理位置的用户可能面临不同的安全风险，模型在多语言环境下的表现不一致。  
  
研究人员表示，DeepSeek-R1模型在Malicious-Educator上的表现很差，拒绝率约为20%。更糟糕的是，由于系统设计存在缺陷，它在安全管理员介入之前就先输出了有害内容。  
  
可能的改进措施  
  
不透明的安全推理  
  
隐藏敏感查询的中间推理步骤，减少攻击者利用CoT机制的机会。  
  
正如首席研究员张建义所言，“展示安全逻辑就像是给黑客一份绕过防御的路线图”。  
  
对抗性训练  
  
在模型训练过程中引入对抗性样本，增强模型对复杂攻击的抵抗力。  
  
研究人员建议，在模型对齐期间模拟H-CoT风格的攻击。  
  
多语言防御  
  
协调跨语言的安全协议，确保模型在多语言环境下的安全性。  
  
研究人员表示，多语言防御是必要的，以确保模型在不同语言环境下的安全性。  
  
实时安全监控  
  
加强生成后审核机制，确保有害内容在输出前被有效拦截。  
  
研究人员指出，DeepSeek-R1的生成后审核存在缺陷，其拒绝率高达20%，导致有害内容在被删除之前短暂浮现。  
  
结论  
  
H-CoT攻击揭示了当前大型推理模型在安全推理机制上的严重漏洞，尤其是在面对复杂和多语言攻击时。研究团队呼吁开发更强大的安全机制，以平衡模型效用与内容无害性，确保AI技术的安全应用。  
  
感兴趣乾可阅读论文原文（参考链接3）。  
  
![](https://mmbiz.qpic.cn/mmbiz_png/0KRmt3K30icUIgGFmqh0BECuuJ3Ma0jEriaLVybgdHQx4tib5XsI8XwouaYECiaXvJA8U1AZYialOer9J0chRAU1g6A/640?wx_fmt=png&from=appmsg "")  
  
![](https://mmbiz.qpic.cn/mmbiz_png/0KRmt3K30icUIgGFmqh0BECuuJ3Ma0jEricymC4qlmOAdPpBCQvenUiab6SJtw8Jb85Q7zEOoicyRia3QhIvxxPYuSQ/640?wx_fmt=png&from=appmsg "")  
  
  
参考资源  
  
1、https://www.govinfosecurity.com/ai-hijacked-new-jailbreak-exploits-chain-of-thought-a-27594  
  
2、https://cybersecuritynews.com/research-jailbreaked-llm-models/  
  
3、https://arxiv.org/pdf/2502.12893  
  
